# PenaltyAttack
Our experiment is based on Python3.11. 
### Requirements
- pytorch
- torchvision
- numpy
- matplotlib
- torchattacks https://github.com/Harry24k/adversarial-attacks-pytorch
- robustbench https://github.com/RobustBench/robustbench
- Adversarial-library https://github.com/jeromerony/adversarial-library

Results for MNIST in Table 3, Table 4 can be generated by running  ```python experiment_mnist_penalty_attack.py```.<br>
Results for CIFAR10 in Table 5, Table 6 can be generated by running  ```python experiment_cifar10_penalty_attack.py```.<br>
Results for ImageNet in Table 7, Table 8 can be generated by running  ```python experiment_imagenet_penalty_attack.py```.<br>
Running results will be stored in ```./result```.<br>
Some generated adversarial examples are in ```./GeneratedAE```<br>
```./torchattacks``` is the torchattacks package we modified for our experiments. <br>
```./plot_curve``` includes the code for plotting the figures of how ASR changes with respect to perturbation.<br> 

Only models for MNIST are in the repository. Models for CIFAR and ImageNet will be downloaded automatically by robustbench package. MNIST and CIFAR datasets will also be downloaded automatically and the images used for ImageNet(ILSVRC2012) are the first 1000 images in validation set. <br>


